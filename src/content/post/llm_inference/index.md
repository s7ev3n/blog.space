---
title: "LLM Inference"
description: "llm inference notes"
publishDate: "13 July 2024"
updatedDate: "1 Feb 2025"
tags: ["tech/llm"]
draft: false
---

> 由于部署和调用LLM模型需求急速增加，迅速催生了LLM推理这一领域，围绕如何加快推理速度和成本首先从学术界出现大量结合系统领域知识的工作，使得MLSys非常火热。本文是学习LLM推理的一些笔记。

## KV Cache